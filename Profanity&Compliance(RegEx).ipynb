{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV6yxhzaTO56"
      },
      "outputs": [],
      "source": [
        "# @title Install & imports\n",
        "!pip -q install pyyaml pandas unidecode\n",
        "\n",
        "import os, re, yaml, json, unicodedata\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", 200)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Upload & extract a .zip of your YAML/JSON folder (simple & robust) ---\n",
        "\n",
        "from google.colab import files\n",
        "import zipfile, io, os, shutil\n",
        "\n",
        "# 1) Upload the ZIP (e.g., All_Conversations.zip)\n",
        "uploaded = files.upload()\n",
        "if not uploaded:\n",
        "    raise SystemExit(\"No file uploaded.\")\n",
        "\n",
        "zip_name = next(iter(uploaded))\n",
        "if not zip_name.lower().endswith(\".zip\"):\n",
        "    raise SystemExit(f\"Expected a .zip file, got: {zip_name}\")\n",
        "\n",
        "# 2) Clean extract target and unzip to /content/data\n",
        "DATA_DIR = \"/content/data\"\n",
        "if os.path.exists(DATA_DIR):\n",
        "    shutil.rmtree(DATA_DIR)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(io.BytesIO(uploaded[zip_name]), \"r\") as z:\n",
        "    z.extractall(DATA_DIR)\n",
        "\n",
        "# 3) Quick sanity: count YAML/JSON (recursively) and show a few\n",
        "CALL_FILE_EXTS = (\".yaml\", \".yml\", \".json\")\n",
        "found_all, found_yaml, found_json = [], [], []\n",
        "\n",
        "for root, dirs, files in os.walk(DATA_DIR):\n",
        "    # skip common noise dirs\n",
        "    dirs[:] = [d for d in dirs if d not in {'.ipynb_checkpoints', '__MACOSX', '.git', '.svn'}]\n",
        "    for f in files:\n",
        "        fl = f.lower()\n",
        "        if fl.endswith(CALL_FILE_EXTS):\n",
        "            p = os.path.join(root, f)\n",
        "            found_all.append(p)\n",
        "            if fl.endswith((\".yaml\", \".yml\")):\n",
        "                found_yaml.append(p)\n",
        "            elif fl.endswith(\".json\"):\n",
        "                found_json.append(p)\n",
        "\n",
        "print(f\"ZIP extracted to: {DATA_DIR}\")\n",
        "print(f\"Found {len(found_all)} transcript file(s): {len(found_json)} JSON, {len(found_yaml)} YAML\")\n",
        "\n",
        "if found_json:\n",
        "    print(\"\\nSample JSON files:\")\n",
        "    for p in sorted(found_json)[:10]:\n",
        "        print(\" -\", p)\n",
        "\n",
        "if found_yaml:\n",
        "    print(\"\\nSample YAML files:\")\n",
        "    for p in sorted(found_yaml)[:10]:\n",
        "        print(\" -\", p)\n",
        "\n",
        "if not found_all:\n",
        "    print(\"\\nNo YAML/JSON files found. Check that your ZIP contains .json/.yaml/.yml files.\\n\"\n",
        "          \"If they’re inside a nested folder, that’s fine—this search is recursive.\")\n"
      ],
      "metadata": {
        "id": "NLTszUiDXyyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Discovery & Loader (JSON, recursive + NFKC helper)\n",
        "import os, re, json, unicodedata, pandas as pd\n",
        "\n",
        "EXCLUDED_DIRS = {\".ipynb_checkpoints\", \"__MACOSX\", \".git\", \".svn\"}\n",
        "JSON_EXTS = (\".json\",)\n",
        "TIME_RX = re.compile(r\"^\\d{1,2}:\\d{2}:\\d{2}(?:\\.\\d+)?$\")\n",
        "\n",
        "def iter_json_files(root: str):\n",
        "    \"\"\"Yield JSON file paths recursively, skipping checkpoint/hidden dirs.\"\"\"\n",
        "    for dirpath, dirnames, filenames in os.walk(root):\n",
        "        dirnames[:] = [d for d in dirnames if d not in EXCLUDED_DIRS and not d.startswith(\".\")]\n",
        "        for fn in filenames:\n",
        "            if fn.lower().endswith(JSON_EXTS) and not fn.startswith(\".\"):\n",
        "                yield os.path.join(dirpath, fn)\n",
        "\n",
        "def _to_seconds(t):\n",
        "    \"\"\"Accept float/int seconds or 'HH:MM:SS(.ms)' → seconds (float).\"\"\"\n",
        "    if isinstance(t, (int, float)):\n",
        "        return float(t)\n",
        "    if isinstance(t, str):\n",
        "        t = t.strip()\n",
        "        if TIME_RX.match(t):\n",
        "            h, m, s = t.split(\":\")\n",
        "            return int(h)*3600 + int(m)*60 + float(s)\n",
        "        try:\n",
        "            return float(t)\n",
        "        except:\n",
        "            return float(\"nan\")\n",
        "    return float(\"nan\")\n",
        "\n",
        "def _norm_speaker(spk: str) -> str:\n",
        "    \"\"\"Map various spellings to 'agent' or 'customer'.\"\"\"\n",
        "    s = (spk or \"\").strip().lower()\n",
        "    if \"agent\" in s: return \"agent\"\n",
        "    if \"customer\" in s or \"caller\" in s or \"borrow\" in s: return \"customer\"\n",
        "    return \"customer\"\n",
        "\n",
        "def nfkc(text: str) -> str:\n",
        "    \"\"\"\n",
        "    NFKC-normalize for predictable processing.\n",
        "    Example: 'I\\\\u2019m' (curly apostrophe) → \"I'm\" (straight apostrophe).\n",
        "    \"\"\"\n",
        "    if text is None: return \"\"\n",
        "    return unicodedata.normalize(\"NFKC\", text)\n",
        "\n",
        "def load_call_json(path, call_id=None):\n",
        "    \"\"\"Load one JSON transcript as a list of normalized utterance dicts.\"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Expect list of utterances; if dict, try common key 'utterances'\n",
        "    items = data\n",
        "    if isinstance(data, dict):\n",
        "        items = data.get(\"utterances\", [])\n",
        "    if not isinstance(items, list):\n",
        "        raise ValueError(f\"Unrecognized JSON structure in {path}\")\n",
        "\n",
        "    utts = []\n",
        "    for i, u in enumerate(items or []):\n",
        "        text_raw = (u.get(\"text\") or u.get(\"utterance\") or u.get(\"content\") or \"\")\n",
        "        utts.append({\n",
        "            \"call_id\": call_id,\n",
        "            \"idx\": i,\n",
        "            \"speaker\": _norm_speaker(u.get(\"speaker\", \"\")),\n",
        "            \"text\": text_raw,             # keep original (as in file)\n",
        "            \"text_nfkc\": nfkc(text_raw),  # NFKC-normalized (for matching)\n",
        "            \"stime\": _to_seconds(u.get(\"stime\")),\n",
        "            \"etime\": _to_seconds(u.get(\"etime\")),\n",
        "        })\n",
        "    # Preserve overlaps; just sort by time\n",
        "    utts.sort(key=lambda r: (r[\"stime\"], r[\"etime\"], r[\"idx\"]))\n",
        "    return utts\n",
        "\n",
        "def load_all_calls(data_dir: str):\n",
        "    \"\"\"Load all JSONs, dedupe by call_id (filename stem), and report ingestion.\"\"\"\n",
        "    seen, rows, loaded, dupes = set(), [], [], []\n",
        "    for path in sorted(iter_json_files(data_dir)):\n",
        "        call_id = os.path.splitext(os.path.basename(path))[0]\n",
        "        if call_id in seen:\n",
        "            dupes.append(path); continue\n",
        "        try:\n",
        "            rows.extend(load_call_json(path, call_id=call_id))\n",
        "            seen.add(call_id); loaded.append(path)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Failed to parse {path}: {e}\")\n",
        "    return pd.DataFrame(rows), loaded, dupes\n",
        "\n",
        "# Run\n",
        "df, files_loaded, dupes = load_all_calls(DATA_DIR)\n",
        "print(\"JSON files loaded:\", len(files_loaded), \"| dupes skipped:\", len(dupes))\n",
        "print(\"Distinct calls:\", df['call_id'].nunique(), \"| Total utterances:\", len(df))\n",
        "df.head(8)\n"
      ],
      "metadata": {
        "id": "6uZF708Db5hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1: Profanity detection\n",
        "\n",
        "# Fixed frequent profanity terms in your data\n",
        "PROFANITY_TERMS = [\n",
        "    \"fuck\", \"shit\", \"bitch\", \"asshole\", \"bastard\", \"damn\", \"crap\",\n",
        "    \"son of a bitch\", \"piece of shit\", \"shut the fuck up\",\n",
        "]\n",
        "\n",
        "# Optional: words that look similar but are benign; helps avoid silly false positives\n",
        "WHITELIST = {\"passion\", \"assess\", \"assignment\", \"scunthorpe\"}\n",
        "\n",
        "import re, os\n",
        "from typing import List, Dict\n",
        "\n",
        "def compile_q1_patterns(terms: List[str]):\n",
        "    \"\"\"Build two lists of regexes: single words and multi-word phrases.\"\"\"\n",
        "    word_patterns = []\n",
        "    phrase_patterns = []\n",
        "    for term in terms:\n",
        "        term_low = term.lower().strip()\n",
        "        if \" \" in term_low:\n",
        "            # Phrase: just join tokens with spaces; transcripts are speech → we expect spaces\n",
        "            tokens = [re.escape(t) for t in term_low.split()]\n",
        "            rx = re.compile(r\"(?<!\\w)\" + r\"\\s+\".join(tokens) + r\"(?!\\w)\", re.I)\n",
        "            phrase_patterns.append(rx)\n",
        "        else:\n",
        "            # Single word with word boundaries\n",
        "            rx = re.compile(rf\"(?<!\\w){re.escape(term_low)}(?!\\w)\", re.I)\n",
        "            word_patterns.append(rx)\n",
        "    return word_patterns, phrase_patterns\n",
        "\n",
        "Q1_WORD_RX, Q1_PHRASE_RX = compile_q1_patterns(PROFANITY_TERMS)\n",
        "\n",
        "def find_profanity_in_text(text_nfkc: str) -> List[Dict]:\n",
        "    \"\"\"Return a list of matches in one utterance (each match has type, start, end, substr).\"\"\"\n",
        "    hits = []\n",
        "    if not text_nfkc:\n",
        "        return hits\n",
        "\n",
        "    # single words\n",
        "    for rx in Q1_WORD_RX:\n",
        "        for m in rx.finditer(text_nfkc):\n",
        "            snippet = text_nfkc[m.start():m.end()]\n",
        "            if snippet.lower() in WHITELIST:\n",
        "                continue\n",
        "            hits.append({\"type\": \"word\", \"start\": m.start(), \"end\": m.end(), \"substr\": snippet})\n",
        "\n",
        "    # phrases\n",
        "    for rx in Q1_PHRASE_RX:\n",
        "        for m in rx.finditer(text_nfkc):\n",
        "            snippet = text_nfkc[m.start():m.end()]\n",
        "            hits.append({\"type\": \"phrase\", \"start\": m.start(), \"end\": m.end(), \"substr\": snippet})\n",
        "\n",
        "    return hits\n",
        "\n",
        "def q1_detect_on_df(df):\n",
        "    \"\"\"Loop over all utterances and record where we found profanity.\"\"\"\n",
        "    records = []\n",
        "    for row in df.itertuples(index=False):\n",
        "        matches = find_profanity_in_text(row.text_nfkc)\n",
        "        if matches:\n",
        "            records.append({\n",
        "                \"call_id\": row.call_id,\n",
        "                \"idx\": row.idx,\n",
        "                \"speaker\": row.speaker,   # 'agent' or 'customer'\n",
        "                \"text\": row.text,         # original text (helps when reading results)\n",
        "                \"hits\": matches\n",
        "            })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "def q1_summarize(hits_df):\n",
        "    \"\"\"Make one row per call: did the agent use profanity? did the customer?\"\"\"\n",
        "    if hits_df.empty:\n",
        "        empty_summary = pd.DataFrame(columns=[\"call_id\",\"agent_used_profanity\",\"customer_used_profanity\"])\n",
        "        return empty_summary, hits_df\n",
        "\n",
        "    tmp = (hits_df.assign(flag=1)\n",
        "                 .pivot_table(index=\"call_id\", columns=\"speaker\", values=\"flag\", aggfunc=\"max\", fill_value=0)\n",
        "                 .rename(columns={\"agent\":\"agent_used_profanity\", \"customer\":\"customer_used_profanity\"})\n",
        "                 .reset_index())\n",
        "\n",
        "    # Ensure both columns exist even if one speaker never matched\n",
        "    for c in [\"agent_used_profanity\",\"customer_used_profanity\"]:\n",
        "        if c not in tmp.columns:\n",
        "            tmp[c] = 0\n",
        "\n",
        "    tmp[\"agent_used_profanity\"] = tmp[\"agent_used_profanity\"].astype(bool)\n",
        "    tmp[\"customer_used_profanity\"] = tmp[\"customer_used_profanity\"].astype(bool)\n",
        "    return tmp, hits_df.sort_values([\"call_id\",\"idx\"])\n",
        "\n",
        "# run Q1\n",
        "q1_hits = q1_detect_on_df(df)\n",
        "q1_summary, q1_details = q1_summarize(q1_hits)\n",
        "\n",
        "# make utterance numbers human-friendly (1-based) ---\n",
        "if not q1_details.empty:\n",
        "    q1_details = q1_details.assign(utterance_no=(q1_details[\"idx\"] + 1).astype(int))\n",
        "    # put the human-facing column up front; keep everything else\n",
        "    preferred = [\"call_id\", \"utterance_no\", \"speaker\", \"text\", \"hits\", \"idx\"]\n",
        "    q1_details = q1_details[[c for c in preferred if c in q1_details.columns]\n",
        "                            + [c for c in q1_details.columns if c not in preferred]]\n",
        "\n",
        "print(\"Q1: calls with AGENT profanity =\", q1_summary.query(\"agent_used_profanity\").shape[0])\n",
        "print(\"Q1: calls with CUSTOMER profanity =\", q1_summary.query(\"customer_used_profanity\").shape[0])\n",
        "display(q1_summary.head(8))\n",
        "\n",
        "# ---- save Q1 outputs\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "q1_summary.to_csv(\"outputs/q1_profanity_summary_by_call.csv\", index=False)\n",
        "q1_details.to_csv(\"outputs/q1_profanity_utterance_details.csv\", index=False)\n",
        "print(\"Saved Q1 to ./outputs/\")\n"
      ],
      "metadata": {
        "id": "fAJeRdqdhMvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: Privacy & Compliance — final regex + state machine:\n",
        "\n",
        "import re, os, pandas as pd\n",
        "from math import inf\n",
        "\n",
        "# Speaker normalization + sorting\n",
        "BORROWER_ALIASES = {\"borrower\", \"customer\", \"caller\", \"client\", \"user\"}\n",
        "\n",
        "def is_agent(spk: str) -> bool:\n",
        "    return (spk or \"\").strip().lower() == \"agent\"\n",
        "\n",
        "def is_borrower(spk: str) -> bool:\n",
        "    return (spk or \"\").strip().lower() in BORROWER_ALIASES\n",
        "\n",
        "def _sf(x):\n",
        "    try: return float(x)\n",
        "    except: return inf  # NaNs/None sort to the end\n",
        "\n",
        "# Agent verification prompts (open \"pending\" window)\n",
        "VERIFY_PROMPT_PATTERNS = [\n",
        "    r\"\\b(verify|verification|confirm|confirmation)\\b.*\\b(date of birth|dob)\\b\",\n",
        "    r\"\\b(verify|confirm)\\b.*\\b(address|street|city|zip|zipcode|zip\\s*code|postal|postcode|pin\\s*code|pincode)\\b\",\n",
        "    r\"\\b(verify|confirm)\\b.*\\b(ssn|social security|last\\s*4|last four)\\b\",\n",
        "    r\"\\b(for (?:security|verification))\\b.*\\b(dob|date of birth|address|ssn|last\\s*4|last four)\\b\",\n",
        "]\n",
        "VERIFY_PROMPT_RX = [re.compile(p, re.I) for p in VERIFY_PROMPT_PATTERNS]\n",
        "\n",
        "# Sensitive disclosures (must NOT occur before verified=True)\n",
        "SENSITIVE_PATTERNS = [\n",
        "    r\"\\bcurrent\\s+balance\\b(?:[^0-9]*|\\s*is\\s*)\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\bbalance\\b(?:[^0-9]*|\\s*is\\s*)\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\bamount(?:\\s*due)?\\b(?:[^0-9]*|\\s*is\\s*)\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\byou\\s+owe\\b.*?\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\bpayment\\s+(?:posted|received)\\b.*?\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\bwe\\s+received(?:\\s+your)?\\s+payment\\b.*?\\$?\\d[\\d,]*(?:\\.\\d{2})?\",\n",
        "    r\"\\b(account|acct|a\\/c)\\s*(?:no\\.?|number|#)?\\s*[:#]?\\s*\\d{3,}\\b\",\n",
        "    r\"\\b(?:ending\\s+in|last\\s*(?:4|four))\\b.*?\\b\\d{4}\\b\",\n",
        "    r\"\\bssn\\b.*?\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b\",\n",
        "]\n",
        "SENSITIVE_RX = [re.compile(p, re.I) for p in SENSITIVE_PATTERNS]\n",
        "\n",
        "# Verification evidence from borrower\n",
        "\n",
        "# Numeric DOBs\n",
        "DOB_NUMERIC_RXES = [\n",
        "    r\"\\b(0?[1-9]|1[0-2])[\\/\\-\\.](0?[1-9]|[12]\\d|3[01])[\\/\\-\\.](\\d{2,4})\\b\",  # MM/DD/YYYY or MM-DD-YY\n",
        "    r\"\\b(0?[1-9]|[12]\\d|3[01])[\\/\\-\\.](0?[1-9]|1[0-2])[\\/\\-\\.](\\d{2,4})\\b\",  # DD/MM/YYYY or DD-MM-YY\n",
        "    r\"\\b(\\d{4})[\\/\\-\\.](0?[1-9]|1[0-2])[\\/\\-\\.](0?[1-9]|[12]\\d|3[01])\\b\",    # YYYY-MM-DD or YYYY.MM.DD\n",
        "]\n",
        "DOB_NUMERIC_RXES = [re.compile(p, re.I) for p in DOB_NUMERIC_RXES]\n",
        "\n",
        "# Month-name DOBs (with/without ordinals, comma, \"of\")\n",
        "MONTH = r\"(jan|feb|mar|apr|may|jun|jul|aug|sep|sept|oct|nov|dec|january|february|march|april|june|july|august|september|october|november|december)\"\n",
        "DAY   = r\"(0?[1-9]|[12]\\d|3[01])(?:st|nd|rd|th)?\"\n",
        "YEAR  = r\"\\d{2,4}\"\n",
        "DOB_TEXT_RXES = [\n",
        "    rf\"\\b{MONTH}\\s+{DAY}\\s*,?\\s*{YEAR}\\b\",           # January 15, 1990 / Jan 15 90\n",
        "    rf\"\\b{DAY}\\s+(?:of\\s+)?{MONTH}\\s*,?\\s*{YEAR}\\b\", # 15th of January 1990 / 15 Jan 90\n",
        "    rf\"\\b{YEAR}\\s+{MONTH}\\s+{DAY}\\b\",                # 1990 January 15\n",
        "]\n",
        "DOB_TEXT_RXES = [re.compile(p, re.I) for p in DOB_TEXT_RXES]\n",
        "\n",
        "# Contiguous 8-digit dates (use only while pending): YYYYMMDD or DDMMYYYY\n",
        "CONTIG_8_RX = re.compile(r\"\\b(\\d{8})\\b\")\n",
        "def _looks_like_contig_date(s: str) -> bool:\n",
        "    m = CONTIG_8_RX.search(s)\n",
        "    if not m: return False\n",
        "    d = m.group(1)\n",
        "    try:  # try YYYYMMDD\n",
        "        yyyy, mm, dd = int(d[0:4]), int(d[4:6]), int(d[6:8])\n",
        "        if 1900 <= yyyy <= 2025 and 1 <= mm <= 12 and 1 <= dd <= 31: return True\n",
        "    except: pass\n",
        "    try:  # try DDMMYYYY\n",
        "        dd, mm, yyyy = int(d[0:2]), int(d[2:4]), int(d[4:8])\n",
        "        if 1900 <= yyyy <= 2025 and 1 <= mm <= 12 and 1 <= dd <= 31: return True\n",
        "    except: pass\n",
        "    return False\n",
        "\n",
        "# SSN / last-4 and guards\n",
        "SSN_FULL_RX    = re.compile(r\"\\b\\d{3}[-\\s]?\\d{2}[-\\s]?\\d{4}\\b\")\n",
        "NINE_DIGIT_RX  = re.compile(r\"\\b\\d{9}\\b\")\n",
        "FOUR_DIGIT_RX  = re.compile(r\"\\b\\d{4}\\b\")\n",
        "MONEY_HINT_RX  = re.compile(r\"\\$|\\b(usd|dollars?)\\b|\\d+,\\d{3}\", re.I)\n",
        "PHONE_HINT_RX  = re.compile(r\"\\b(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})\\b\")\n",
        "TOO_LONG_RX    = re.compile(r\"\\d{10,}\")  # >9 contiguous digits\n",
        "\n",
        "# Address recognition\n",
        "STREET_LINE_RX = re.compile(\n",
        "    r\"\"\"\n",
        "    \\b\n",
        "    \\d{1,6}[A-Za-z]?                           # house number (e.g., 12 or 12A)\n",
        "    (?:\\s+(?:N|S|E|W|NE|NW|SE|SW)\\b\\.?)?       # optional direction\n",
        "    \\s+[A-Za-z0-9][A-Za-z0-9'’\\-\\.]*           # first street token (Elm, O'Neil)\n",
        "    (?:\\s+[A-Za-z0-9'’\\-\\.]+){0,4}             # optional extra name tokens\n",
        "    \\s+\n",
        "    (?:st|street|rd|road|ave|avenue|blvd|boulevard|dr|drive|ln|lane|\n",
        "       ct|court|pl|place|ter|terrace|cir|circle|way|pkwy|parkway|hwy|highway)\\.?\n",
        "    \\b\n",
        "    \"\"\", re.I | re.VERBOSE\n",
        ")\n",
        "\n",
        "ADDRESS_HINT_RX = re.compile(\n",
        "    r\"\\b(\"\n",
        "    r\"street|st\\.?|ave|avenue|blvd|boulevard|road|rd\\.?|drive|dr\\.?|lane|ln\\.?|terrace|ter\\.?|court|ct\\.?|place|pl\\.?|\"\n",
        "    r\"way|circle|cir\\.?|parkway|pkwy\\.?|highway|hwy\\.?|\"\n",
        "    r\"apt|apartment|unit|suite|ste\\.?|bldg|building|fl|floor|#\\s*\\w+|\"\n",
        "    r\"po\\s*box|p\\.?o\\.?\\s*box|box\\s*\\d+|\"\n",
        "    r\"\\d{5}(?:-\\d{4})?|zip|zipcode|zip\\s*code|postal|postcode|pin\\s*code|pincode|\"\n",
        "    r\"city|state\"\n",
        "    r\")\\b\", re.I\n",
        ")\n",
        "\n",
        "def borrower_provided_verification(txt: str, *, pending_only: bool = True) -> bool:\n",
        "    t = (txt or \"\").strip()\n",
        "\n",
        "    # 1) DOB (numeric / month-name / contiguous 8-digit while pending)\n",
        "    if any(rx.search(t) for rx in DOB_NUMERIC_RXES): return True\n",
        "    if any(rx.search(t) for rx in DOB_TEXT_RXES):    return True\n",
        "    if pending_only and _looks_like_contig_date(t):  return True\n",
        "\n",
        "    # 2) SSN (formatted) or bare 9 digits (not money/phone/too-long)\n",
        "    if SSN_FULL_RX.search(t): return True\n",
        "    if pending_only and NINE_DIGIT_RX.search(t):\n",
        "        if not (MONEY_HINT_RX.search(t) or PHONE_HINT_RX.search(t) or TOO_LONG_RX.search(t)):\n",
        "            return True\n",
        "\n",
        "    # 3) Last-4 as bare 4 digits (answer-like, only while pending)\n",
        "    if pending_only and (FOUR_DIGIT_RX.fullmatch(t) or re.fullmatch(r\"\\d{4}\\D{0,2}\", t)):\n",
        "        return True\n",
        "\n",
        "    # 4) Address: strong street line first; then general hints\n",
        "    if STREET_LINE_RX.search(t): return True\n",
        "    if ADDRESS_HINT_RX.search(t): return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Q2 core checker\n",
        "\n",
        "def q2_check_one_call(utterances):\n",
        "    \"\"\"\n",
        "    Input: list of utterance dicts with keys:\n",
        "           speaker, text, text_nfkc, stime, etime, idx\n",
        "    Output: dict { non_compliant: bool, verified_observed: bool, violations: [..] }\n",
        "    \"\"\"\n",
        "    verified = False\n",
        "    pending_verify = False\n",
        "    violations = []\n",
        "\n",
        "    # robust ordering\n",
        "    utts = sorted(utterances, key=lambda r: (_sf(r.get(\"stime\")), _sf(r.get(\"etime\")), r.get(\"idx\", 0)))\n",
        "\n",
        "    for u in utts:\n",
        "        spk = (u.get(\"speaker\") or \"\").lower()\n",
        "        txt = (u.get(\"text_nfkc\") or u.get(\"text\") or \"\")\n",
        "\n",
        "        # 1) Agent prompts → open verification window\n",
        "        if is_agent(spk) and any(rx.search(txt) for rx in VERIFY_PROMPT_RX):\n",
        "            pending_verify = True\n",
        "            continue\n",
        "\n",
        "        # 2) Borrower provides verification during pending window\n",
        "        if is_borrower(spk) and pending_verify:\n",
        "            if borrower_provided_verification(txt, pending_only=True):\n",
        "                verified = True\n",
        "                pending_verify = False\n",
        "                continue\n",
        "\n",
        "        # 3) Sensitive disclosure before verified → violation\n",
        "        if is_agent(spk) and any(rx.search(txt) for rx in SENSITIVE_RX):\n",
        "            if not verified:\n",
        "                violations.append({\n",
        "                    \"idx\": u.get(\"idx\", -1),\n",
        "                    \"text\": u.get(\"text\", txt),\n",
        "                    \"reason\": \"sensitive_before_verification\"\n",
        "                })\n",
        "\n",
        "    return {\n",
        "        \"non_compliant\": bool(violations),\n",
        "        \"verified_observed\": verified,\n",
        "        \"violations\": violations\n",
        "    }\n",
        "\n",
        "def q2_detect_on_df(df: pd.DataFrame):\n",
        "    results, details = [], []\n",
        "    for call_id, g in df.groupby(\"call_id\", sort=True):\n",
        "        utts = g.sort_values([\"stime\",\"etime\",\"idx\"]).to_dict(\"records\")\n",
        "        res = q2_check_one_call(utts)\n",
        "        results.append({\n",
        "            \"call_id\": call_id,\n",
        "            \"non_compliant\": res[\"non_compliant\"],\n",
        "            \"verified_observed\": res[\"verified_observed\"],\n",
        "        })\n",
        "        for v in res[\"violations\"]:\n",
        "            details.append({\"call_id\": call_id, **v})\n",
        "    return pd.DataFrame(results), pd.DataFrame(details)\n",
        "\n",
        "# Run + Save\n",
        "q2_summary, q2_details = q2_detect_on_df(df)\n",
        "\n",
        "if not q2_details.empty:\n",
        "    q2_details = q2_details.assign(utterance_no=(q2_details[\"idx\"] + 1).astype(int))\n",
        "    preferred = [\"call_id\", \"utterance_no\", \"reason\", \"text\", \"idx\"]\n",
        "    q2_details = q2_details[[c for c in preferred if c in q2_details.columns]\n",
        "                            + [c for c in q2_details.columns if c not in preferred]]\n",
        "\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "q2_summary.to_csv(\"outputs/q2_privacy_noncompliance_by_call.csv\", index=False)\n",
        "q2_details.to_csv(\"outputs/q2_privacy_violation_details.csv\", index=False)\n",
        "\n",
        "print(\"Q2: non-compliant calls =\", int(q2_summary.query(\"non_compliant\").shape[0]))"
      ],
      "metadata": {
        "id": "MZgy6tItTywL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final merge → single CSV for Streamlit ===\n",
        "# Produces: outputs/final_calls_review.csv\n",
        "\n",
        "import os, glob, json, re\n",
        "import pandas as pd\n",
        "\n",
        "# --- CONFIG ---\n",
        "DATA_DIR = \"./data\"                 # folder with original JSON calls (set to None if unavailable)\n",
        "OUT_DIR  = \"./outputs\"\n",
        "FINAL_CSV = os.path.join(OUT_DIR, \"regex_summary.csv\")\n",
        "\n",
        "Q1_SUMMARY = os.path.join(OUT_DIR, \"q1_profanity_summary_by_call.csv\")\n",
        "Q1_DETAILS = os.path.join(OUT_DIR, \"q1_profanity_utterance_details.csv\")\n",
        "Q2_SUMMARY = os.path.join(OUT_DIR, \"q2_privacy_noncompliance_by_call.csv\")\n",
        "Q2_DETAILS = os.path.join(OUT_DIR, \"q2_privacy_violation_details.csv\")\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def call_id_from_path(p: str) -> str:\n",
        "    b = os.path.basename(p)\n",
        "    return os.path.splitext(b)[0]\n",
        "\n",
        "# --- 1) Get the authoritative list of call_ids (aim for the full 249) ---\n",
        "master_call_ids = []\n",
        "\n",
        "if DATA_DIR and os.path.isdir(DATA_DIR):\n",
        "    json_paths = []\n",
        "    for pat in (\"**/*.json\", \"*.json\"):\n",
        "        json_paths.extend(glob.glob(os.path.join(DATA_DIR, pat), recursive=True))\n",
        "    master_call_ids = sorted({call_id_from_path(p) for p in json_paths})\n",
        "\n",
        "# If we don't have DATA_DIR or it’s empty, fall back to union from existing CSVs\n",
        "dfs_present = []\n",
        "for p in (Q1_SUMMARY, Q1_DETAILS, Q2_SUMMARY, Q2_DETAILS):\n",
        "    if os.path.isfile(p):\n",
        "        dfs_present.append(pd.read_csv(p))\n",
        "\n",
        "if not master_call_ids and dfs_present:\n",
        "    ids = set()\n",
        "    for d in dfs_present:\n",
        "        # Handle either `call_id` present or nested columns\n",
        "        if \"call_id\" in d.columns:\n",
        "            ids.update(d[\"call_id\"].dropna().astype(str))\n",
        "    master_call_ids = sorted(ids)\n",
        "\n",
        "# Wrap in a starter DataFrame\n",
        "base = pd.DataFrame({\"call_id\": master_call_ids}).astype({\"call_id\": \"string\"})\n",
        "\n",
        "# --- 2) Load Q1 (profanity) summary & details ---\n",
        "q1_sum = pd.read_csv(Q1_SUMMARY) if os.path.isfile(Q1_SUMMARY) else pd.DataFrame(columns=[\"call_id\",\"agent_used_profanity\",\"customer_used_profanity\"])\n",
        "q1_sum = q1_sum.rename(columns={\n",
        "    \"agent_used_profanity\": \"profanity_agent\",\n",
        "    \"customer_used_profanity\": \"profanity_customer\",\n",
        "})\n",
        "for col in (\"profanity_agent\",\"profanity_customer\"):\n",
        "    if col not in q1_sum.columns: q1_sum[col] = False\n",
        "q1_sum[\"call_id\"] = q1_sum.get(\"call_id\",\"\").astype(\"string\")\n",
        "\n",
        "# Utterance numbers that had profanity (from details)\n",
        "q1_det = pd.read_csv(Q1_DETAILS) if os.path.isfile(Q1_DETAILS) else pd.DataFrame(columns=[\"call_id\",\"utterance_no\"])\n",
        "if not q1_det.empty:\n",
        "    # Collect all profanity utterance_no per call, sorted unique, as comma-separated string\n",
        "    hits_by_call = (\n",
        "        q1_det.dropna(subset=[\"call_id\",\"utterance_no\"])\n",
        "             .assign(utterance_no=lambda d: d[\"utterance_no\"].astype(int))\n",
        "             .sort_values([\"call_id\",\"utterance_no\"])\n",
        "             .groupby(\"call_id\")[\"utterance_no\"]\n",
        "             .apply(lambda s: \",\".join(map(str, sorted(set(s.tolist())))))\n",
        "             .reset_index()\n",
        "             .rename(columns={\"utterance_no\": \"profanity_utterance\"})\n",
        "    )\n",
        "else:\n",
        "    hits_by_call = pd.DataFrame(columns=[\"call_id\",\"profanity_utterance\"])\n",
        "hits_by_call[\"call_id\"] = hits_by_call.get(\"call_id\",\"\").astype(\"string\")\n",
        "\n",
        "# --- 3) Load Q2 (privacy/compliance) summary & details ---\n",
        "q2_sum = pd.read_csv(Q2_SUMMARY) if os.path.isfile(Q2_SUMMARY) else pd.DataFrame(columns=[\"call_id\",\"non_compliant\",\"verified_observed\"])\n",
        "q2_sum[\"call_id\"] = q2_sum.get(\"call_id\",\"\").astype(\"string\")\n",
        "if \"verified_observed\" not in q2_sum.columns:\n",
        "    q2_sum[\"verified_observed\"] = False\n",
        "if \"non_compliant\" not in q2_sum.columns:\n",
        "    q2_sum[\"non_compliant\"] = False\n",
        "\n",
        "q2_det = pd.read_csv(Q2_DETAILS) if os.path.isfile(Q2_DETAILS) else pd.DataFrame(columns=[\"call_id\",\"reason\"])\n",
        "q2_det[\"call_id\"] = q2_det.get(\"call_id\",\"\").astype(\"string\")\n",
        "\n",
        "# Info shared before verification → look for reason tokens that indicate pre-verification disclosure\n",
        "PREVERIF_TOKENS = (\n",
        "    \"sensitive_before_verification\",\n",
        "    \"before_verification\",\n",
        "    \"disclosure_before_verification\",\n",
        ")\n",
        "if not q2_det.empty and \"reason\" in q2_det.columns:\n",
        "    preverif = (\n",
        "        q2_det.assign(reason=q2_det[\"reason\"].astype(str).str.lower())\n",
        "              .assign(flag=lambda d: d[\"reason\"].apply(lambda r: any(tok in r for tok in PREVERIF_TOKENS)))\n",
        "              .groupby(\"call_id\")[\"flag\"].any()\n",
        "              .reset_index()\n",
        "              .rename(columns={\"flag\": \"info_shared_without_identity_verification\"})\n",
        "    )\n",
        "else:\n",
        "    preverif = pd.DataFrame(columns=[\"call_id\",\"info_shared_without_identity_verification\"])\n",
        "preverif[\"call_id\"] = preverif.get(\"call_id\",\"\").astype(\"string\")\n",
        "if \"info_shared_without_identity_verification\" not in preverif.columns:\n",
        "    preverif[\"info_shared_without_identity_verification\"] = False\n",
        "\n",
        "# --- 4) Merge everything LEFT onto the full call list ---\n",
        "final = base.merge(q1_sum[[\"call_id\",\"profanity_agent\",\"profanity_customer\"]], on=\"call_id\", how=\"left\")\n",
        "final = final.merge(hits_by_call[[\"call_id\",\"profanity_utterance\"]], on=\"call_id\", how=\"left\")\n",
        "final = final.merge(q2_sum[[\"call_id\",\"verified_observed\",\"non_compliant\"]], on=\"call_id\", how=\"left\")\n",
        "final = final.merge(preverif[[\"call_id\",\"info_shared_without_identity_verification\"]], on=\"call_id\", how=\"left\")\n",
        "\n",
        "# --- 5) Derive the three Q2 flags you requested ---\n",
        "final[\"verified_identity\"] = final[\"verified_observed\"].fillna(False)\n",
        "\n",
        "# If any pre-verification disclosure detected → True\n",
        "final[\"info_shared_without_identity_verification\"] = final[\"info_shared_without_identity_verification\"].fillna(False)\n",
        "\n",
        "# \"no_info_or_identity_shared\":\n",
        "# True when:\n",
        "#   - no verification observed, AND\n",
        "#   - no pre-verification info shared, AND\n",
        "#   - not otherwise marked non_compliant\n",
        "final[\"no_info_or_identity_shared\"] = (\n",
        "    (~final[\"verified_identity\"]) &\n",
        "    (~final[\"info_shared_without_identity_verification\"]) &\n",
        "    (~final[\"non_compliant\"].fillna(False))\n",
        ")\n",
        "\n",
        "# --- 6) Clean up booleans & blanks ---\n",
        "for col in [\"profanity_agent\",\"profanity_customer\",\"verified_identity\",\n",
        "            \"info_shared_without_identity_verification\",\"no_info_or_identity_shared\"]:\n",
        "    if col not in final.columns:\n",
        "        final[col] = False\n",
        "    final[col] = final[col].fillna(False).astype(bool)\n",
        "\n",
        "final[\"profanity_utterance\"] = final[\"profanity_utterance\"].fillna(\"\").astype(str)\n",
        "\n",
        "# Optional: drop helper columns\n",
        "final = final.drop(columns=[\"verified_observed\",\"non_compliant\"], errors=\"ignore\")\n",
        "\n",
        "# --- 7) Save ---\n",
        "final = final.sort_values(\"call_id\")\n",
        "final.to_csv(FINAL_CSV, index=False)\n",
        "\n",
        "print(f\"Saved: {FINAL_CSV}\")\n",
        "print(f\"Rows (calls): {len(final)}\")\n",
        "print(final.head(10))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FU272iRzceEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}